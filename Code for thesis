#PCC code
import matplotlib.pyplot as plt
import seaborn as sns
correlation_matrix = combined_data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Pearson Correlation Matrix of GICS Industry Indices ')
plt.show()

# DCC GARCH Code
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from arch import arch_model


garch_models = {}
standardized_residuals = pd.DataFrame()

for column in combined_data.columns:
   
    garch = arch_model(combined_data[column].dropna(), vol='Garch', p=1, q=1).fit(disp='off')
    
    standardized_residuals[column] = garch.std_resid


def dcc_qml(params, std_residuals):
    T, n = std_residuals.shape
    alpha, beta = params
    Q_bar = np.cov(std_residuals, rowvar=False)
    Q_t = Q_bar.copy()
    log_likelihood = 0

    for t in range(T):
        z_t = std_residuals[t].reshape(-1, 1)
        Q_t = (1 - alpha - beta) * Q_bar + alpha * (z_t @ z_t.T) + beta * Q_t
        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))
        R_t = D_t @ Q_t @ D_t

        log_likelihood += np.log(np.linalg.det(R_t)) + z_t.T @ np.linalg.inv(R_t) @ z_t

    return -log_likelihood[0][0]  


initial_params = np.array([0.01, 0.98])


constraints = (
    {'type': 'ineq', 'fun': lambda x: 1 - x[0] - x[1]},
    {'type': 'ineq', 'fun': lambda x: x[0]},
    {'type': 'ineq', 'fun': lambda x: x[1]}
)


result = minimize(lambda params: dcc_qml(params, standardized_residuals.values),
                  initial_params, method='SLSQP', constraints=constraints)


alpha_opt, beta_opt = result.x
print(f"Optimized α: {alpha_opt}, Optimized β: {beta_opt}")


def calculate_dcc_with_params(std_residuals, alpha, beta):
    T, n = std_residuals.shape
    Q_bar = np.cov(std_residuals, rowvar=False)
    Q_t = Q_bar.copy()
    R_t = np.zeros((n, n, T))

    for t in range(T):
        z_t = std_residuals[t].reshape(-1, 1)
        Q_t = (1 - alpha - beta) * Q_bar + alpha * (z_t @ z_t.T) + beta * Q_t
        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))
        R_t[:, :, t] = D_t @ Q_t @ D_t

    return R_t


dcc_matrix = calculate_dcc_with_params(standardized_residuals.values, alpha_opt, beta_opt)


last_dcc_matrix = dcc_matrix[:, :, -1]

plt.figure(figsize=(12, 8))
sns.heatmap(last_dcc_matrix, annot=True, cmap='coolwarm', center=0.5, fmt=".2f",
            xticklabels=combined_data.columns,
            yticklabels=combined_data.columns,
            vmin=0, vmax=1)
plt.title('DCC Matrix with Industry Labels (Macro) at Last Time Point')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from arch import arch_model
from scipy.optimize import minimize


garch_models = {}
standardized_residuals = pd.DataFrame()

for column in combined_data.columns:
    
    garch = arch_model(combined_data[column].dropna(), vol='Garch', p=1, q=1).fit(disp='off')
   
    standardized_residuals[column] = garch.std_resid


standardized_residuals.index = pd.to_datetime(standardized_residuals.index)


def dcc_qml(params, std_residuals):
    T, n = std_residuals.shape
    alpha, beta = params
    Q_bar = np.cov(std_residuals, rowvar=False)
    Q_t = Q_bar.copy()
    log_likelihood = 0

    for t in range(T):
        z_t = std_residuals[t].reshape(-1, 1)
        Q_t = (1 - alpha - beta) * Q_bar + alpha * (z_t @ z_t.T) + beta * Q_t
        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))
        R_t = D_t @ Q_t @ D_t

        log_likelihood += np.log(np.linalg.det(R_t)) + z_t.T @ np.linalg.inv(R_t) @ z_t

    return -log_likelihood[0][0]  


initial_params = np.array([0.01, 0.98])


constraints = (
    {'type': 'ineq', 'fun': lambda x: 1 - x[0] - x[1]},
    {'type': 'ineq', 'fun': lambda x: x[0]},
    {'type': 'ineq', 'fun': lambda x: x[1]}
)


result = minimize(lambda params: dcc_qml(params, standardized_residuals.values),
                  initial_params, method='SLSQP', constraints=constraints)


alpha_opt, beta_opt = result.x
print(f"Optimized α: {alpha_opt}, Optimized β: {beta_opt}")


def calculate_dcc_with_params(std_residuals, alpha, beta):
    T, n = std_residuals.shape
    Q_bar = np.cov(std_residuals, rowvar=False)
    Q_t = Q_bar.copy()
    R_t = np.zeros((n, n, T))

    for t in range(T):
        z_t = std_residuals[t].reshape(-1, 1)
        Q_t = (1 - alpha - beta) * Q_bar + alpha * (z_t @ z_t.T) + beta * Q_t
        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))
        R_t[:, :, t] = D_t @ Q_t @ D_t

    return R_t


dcc_matrix = calculate_dcc_with_params(standardized_residuals.values, alpha_opt, beta_opt)


index_2020 = standardized_residuals.index.get_indexer([pd.Timestamp('2020-01-01')], method='nearest')[0]
index_2022 = standardized_residuals.index.get_indexer([pd.Timestamp('2021-07-28')], method='nearest')[0]

dcc_matrix_2020 = dcc_matrix[:, :, index_2020]
dcc_matrix_2022 = dcc_matrix[:, :, index_2022]


plt.figure(figsize=(12, 8))
sns.heatmap(dcc_matrix_2020, annot=True, cmap='coolwarm', center=0.5, fmt=".2f",
            xticklabels=combined_data.columns,
            yticklabels=combined_data.columns,
            vmin=0, vmax=1)
plt.title('DCC Matrix with Industry Labels at 2020-01-01')
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(dcc_matrix_2022, annot=True, cmap='coolwarm', center=0.5, fmt=".2f",
            xticklabels=combined_data.columns,
            yticklabels=combined_data.columns,
            vmin=0, vmax=1)
plt.title('DCC Matrix with Industry Labels at 2021-07-28')
plt.show()

# Distance Correlation
import numpy as np
import dcor
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = combined_data.values  
columns = combined_data.columns


dist_corr_matrix = np.zeros((data.shape[1], data.shape[1]))

for i in range(data.shape[1]):
    for j in range(data.shape[1]):
        dist_corr_matrix[i, j] = dcor.distance_correlation(data[:, i], data[:, j])


dist_corr_df = pd.DataFrame(dist_corr_matrix, index=columns, columns=columns)


plt.figure(figsize=(12, 8))
sns.heatmap(dist_corr_df, annot=True, cmap='coolwarm', center=0.5, fmt=".2f",
            xticklabels=columns,
            yticklabels=columns,
            vmin=0, vmax=1)
plt.title('Distance Correlation Matrix with Industry Labels')
plt.show()

#PACF for Macroeconomic data
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_pacf

# Step 1:
group1_sectors = ['Information Technology']
group2_sectors = ['Health Care', 'Consumer Discretionary']
group3_sectors = [sector for sector in aligned_combined_data.columns if sector not in group1_sectors + group2_sectors]

# Step 2:
group1_prices = aligned_combined_data[group1_sectors].sum(axis=1)
group2_prices = aligned_combined_data[group2_sectors].sum(axis=1)
group3_prices = aligned_combined_data[group3_sectors].sum(axis=1)

price_groups = {
    'Group 1': group1_prices,
    'Group 2': group2_prices,
    'Group 3': group3_prices
}

for group_name, group_prices in price_groups.items():
    for macro_col in aligned_smoothed_macro_data.columns:
        plt.figure(figsize=(10, 6))

        
        combined_series = group_prices.diff().dropna() - aligned_smoothed_macro_data[macro_col].diff().dropna()

      
        max_lags = min(60, len(combined_series) // 2 - 1)

        
        plot_pacf(combined_series, lags=max_lags, title=f'PACF - {group_name} vs {macro_col}', zero=False)
        plt.suptitle(f'PACF Analysis: {group_name} vs {macro_col}', fontsize=16)
        plt.show()

#PACF for sentimental data
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_pacf
merged_data = merged_data.sort_values(by='Date')


price_data = merged_data.pivot_table(index='Date', columns='Sector', values='Price')
sentiment_data = merged_data.pivot_table(index='Date', columns='Sector', values='Sentiment Score')
pairs_to_analyze = {
    'Communication Services Social Media': ['Consumer Discretionary','Health Care','Real Estate','Utlities'],
    'Information Technology Social Media': ['Consumer Staples'],
    'Utilities Social Media': ['Utilities']
}
total_plots = sum(len(sectors) for sectors in pairs_to_analyze.values())
fig, axes = plt.subplots(total_plots, 1, figsize=(15, 3 * total_plots))
fig.tight_layout(pad=3.0)
plot_idx = 0
for social_media_label, sectors in pairs_to_analyze.items():
    for sector in sectors:
      
        social_media_column = social_media_label.replace(' Social Media', '').strip()

        if social_media_column not in sentiment_data.columns:
            print(f"Column '{social_media_column}' not found in sentiment data.")
            continue
        if sector not in price_data.columns:
            print(f"Column '{sector}' not found in price data.")
            continue

        
        ax = axes[plot_idx]
        plot_pacf(sentiment_data[social_media_column], ax=ax, title=f'PACF of {social_media_label} vs {sector} Price', lags=20)
        ax.set_xlabel('Lags')
        ax.set_ylabel('Partial Autocorrelation')
        plot_idx += 1


if plot_idx < total_plots:
    for idx in range(plot_idx, total_plots):
        fig.delaxes(axes[idx])

plt.suptitle('PACF Plots for Selected Strongly Correlated Price and Social Media Sentiment Data by Sector', y=1.02, fontsize=16)
plt.show()
